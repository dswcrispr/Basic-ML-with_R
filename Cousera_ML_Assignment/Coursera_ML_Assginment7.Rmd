---
title: "K-means Clustering and PCA"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this exercise, we will implement the K-means clustering algorithm and apply it to compress an image.

In the second part, we will use principal component analysis to find a low-dimensional representation of face images. 


# 1.K-means Clustering

### 1.1 Implementing K-means

#### 1.1.1 Data loading
``` {r dataloading}
# Suppressing warning message
options(warn = -1)

# library for loading matlab file
library(rmatio) 
data = read.mat("C:/Users/user/Documents/Basic-ML-with_R/data/ex7data2.mat")
list2env(data, .GlobalEnv)
rm(data)
```

#### 1.1.2  Finding closest centroids

Using 'findClosestCentroids' function, we assigns every training examples x(i) to its closest centroid, given the current positions.

``` {r closestcentroid}
findClosestCentroids = function(X, centroids) {
  
  m = dim(X)[1]
  c = rep(0, m)
  k = dim(centroids)[1]
  
  for (i in 1:m){
    # Calculate distances between sample x and each centroid
    X_i_matrix = rep(1, k) %*% t(X[i, ])
    diff = X_i_matrix - centroids # K by n, where n is the dimension of X
    diff_2 = diff^2
    distance = apply(diff_2, 1, sum) 
    # Assign every training example to closest centroid's index
    c[i] = which.min(distance)
  }
  # return c
  c
}
```

#### 1.1.3  Computing centroid means

Using 'computeCentroids' function, we recomputes, for each centroid, the mean of the points that were assigned to it.

``` {r computecentroid}
computeCentroids = function(X, idx, K){
  
  n = dim(X)[2]
  centroids = matrix(0, nrow = K, ncol = n)
    
  for (i in 1:K){
    centroids[i, ] = apply(X[which(idx == i), ], 2, mean)
  }
  centroids
}
```

#### 1.1.4  K-means on example dataset

We use 'runKmeans' function to implement K-means algorithm with iterations.
``` {r settings}
ini_cen = matrix(c(3, 3, 6, 2, 8, 5), nrow = 3, byrow = TRUE)
max_iter = 10
```

Make 'runKmeans' function first 
```{r kmenaalogo}
runKmeans = function(X, init_centroids, max_iter){
  
  # Settings
  m = dim(X)[1]
  n = dim(X)[2]
  K = dim(init_centroids)[1]
  
  previous_centroids = array(0, dim = c(dim(init_centroids), max_iter +1))
  previous_centroids[, , 1] = init_centroids
  
  # Run K-means algorithm with iterations 
  for (i in 1:max_iter){
    idx = findClosestCentroids(X, previous_centroids[, , i])
    previous_centroids[, , i + 1] = computeCentroids(X, idx, K)
  }
  list(centroids = previous_centroids[, , max_iter + 1], idx = idx)
}
```

Run K-means algorithm
``` {r runkmeanalgo}
K_means_algorithm = runKmeans(X, ini_cen, 10)

# Save results
centroids = K_means_algorithm$centroids
idx = K_means_algorithm$idx
```

Visualize the results
``` {r visualization}
# Convert matrix to data.frame first
data_plot = data.frame(X, idx)
centroids_plot = as.data.frame(centroids)

# Create plot
library(ggplot2)
p = ggplot(data_plot, aes(X1, X2), color = idx) + geom_point(aes(color = idx))+
  geom_point(data = centroids_plot, aes(V1, V2), color = 'red', pch = 4,
             size = 3, lwd = 2) +
  ylab("X2") + xlab("X1") +
  scale_x_continuous(breaks = seq(-1, 9, 1)) +
  scale_y_continuous(breaks = seq(0, 6, 1)) +
  theme_bw() + ggtitle("The result of K-means algorithm") +
  theme(legend.title = element_blank(), 
        panel.grid.major.x =element_blank(),
        panel.grid.minor.y = element_blank(),  
        panel.grid.minor.x = element_blank(), 
        panel.grid.major.y = element_blank()) +
  theme(legend.position = 'none')
p
```

### 1.2 Image compression with K-means


Here, we apply K-means to image compression.

In a straightforward 24-bit color representation of an image, each pixel is represented as three 8-bit unsigned integers(ranging form 0 to 255) that specify the red, green and blue intensity values. This encoding is often refered to as the RGB encoding.

Our image contains thousands of colors, and in this part of the exercise, we will reduce the number of colors to 16 colors. By making this reduction, it is possible to represent the photo in an efficient way. Specifically, we only need to store the RGB values of the 16 selected colors, and for each pixel in the image we now need to only store the index of the color at that location.

In this exercise, we will use the K-means algorithm to select the 16 colors that will be used to represent the compressed image. Concretely, we will treat every pixel in the original image as a data example and use the K-means algorithm to find the 16 colors that best group(cluster) the pixels in the 3-dimensional RGB space.

Once we have computed the cluster centroids on the image, we will then use the 16 colors to replace the pixels in the orginal image.

#### 1.2.1 Data loading and settings
``` {r dataloading2}
data = read.mat("C:/Users/user/Documents/Basic-ML-with_R/data/bird_small.mat")
list2env(data, .GlobalEnv)
rm(data)



# For example, A[50,50,3] gives the blue intensity of the pixel at row 50 and column 50.  
A[50,50,3]

# Divide by 255 so that all values are in the range 0 -1
A = A / 255

# Reshape A to create an m by 3 matrix X of pixel colors(m = 128 * 128)
# Each row will contain the Red, Green and Bule pixel values.
# This gives us our dataset matrix X that we will use K-means on.
X = matrix(A, dim(A)[1] * dim(A)[2], 3)
```


#### 1.2.2 Run K-means algoritm
``` {r kmeanimage}
K = 16
max_iters = 10

# Initialize the centroids randomly first

# Function loading
source("C:/Users/user/Documents/Basic-ML-with_R/function/InitCentroids.R")

initial_centroids = InitCentroids(X, K)

# Implemnts K-means
K_means = runKmeans(X, initial_centroids, K)

# save results
centroids = K_means$centroids
idx = K_means$idx
```

After finding the top K = 16 colors to represent the image, we can now assign each pixel to its closest centroid. This allows us to represent the original image using the centroid assignments of each pixel.

``` {r imagecompress}
# Image compression
X_recovered = centroids[idx, ] 

# Reshape the recovered image into array
X_recovered = array(X_recovered, c(128, 128, 3))
```

#### 1.2.3 Display images
``` {r display image}

# Loading 'raster' package first
library(raster)


# Dismplay
op = par(mfrow = c(1, 2), mar = c(5, 4, 4, 2) + 0.1)

b = brick(A)
plotRGB(b, stretch = 'lin', asp = 2, axes = TRUE, main = "Original")

b = brick(X_recovered)
plotRGB(b, stretch = 'lin', asp = 2, axes = TRUE, main = "Compressed, with 16 colors")

par(op)
```
Finally, we can view he effects of the compression by reconstructuring the image based only on the centroid assignments.



# 2.Principal Component Analysis

We will first experiment with an example 2D dataset to get intuition on how PCA works and then use it on a bigger dataset of 5000 face image dataset.


### 2.1 Example dataset
``` {r dataset}
data = read.mat("C:/Users/user/Documents/Basic-ML-with_R/data/ex7data1.mat")
list2env(data, .GlobalEnv)
rm(data)
```

#### 2.1.1 Visualization
``` {r visu}
example_dataset = data.frame(V1 = X[, 1], V2 = X[, 2])
p1 = ggplot(example_dataset, aes(V1, V2)) + geom_point(color = 'blue') +
  scale_x_continuous(breaks = seq(0, 7, 1)) + 
  scale_y_continuous(breaks = seq(2, 8, 1)) + theme_bw() +
  ggtitle("Example Dataset 1") +
  theme(legend.title = element_blank(), panel.grid.major.x = element_blank() ,
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank())
p1
```

### 2.2 Implementing PCA

PCA consists of two computational steps:
First, we compute the covariance matrix of the data.
Second, we use 'svd' function to compute the eigenvectors. These will correspond to the principal components of variation in the data.

Before using PCA, it is important to first normalize the data. we use 'featureNormalize' function that we made in EX1.

#### 2.2.1 Normalize data
``` {r normal}
# Function loading
source("C:/Users/user/Documents/Basic-ML-with_R/function/featureNormalize.R")

normalization = featureNormalize(X)
X_norm = normalization$X_norm
```

#### 2.2.2 Make 'PCA' function and implement
``` {r pcaimple}
PCA = function(X) {
  
  m = dim(X)[1]
  n = dim(X)[2]
  
  # Make variance-covariance matrix for X
  v_c_matrix = (t(X) %*% X) / m
  
  # Eigenvalue decomposition v_c_matrix
  eigendecomp = svd(v_c_matrix) # svd function returns the eigenvectors u,
                                # the eigenvalues in d as a list.
  eigendecomp
}

# Implements PCA function
eigendecomp = PCA(X_norm)

# Save the results
principal_components = eigendecomp$u
variation = eigendecomp$d

# Show top principal component
sprintf('Top principal component: %.3f, %.3f', principal_components[1, 1],
        principal_components[2, 1])
```

### 2.3 Dimensionality reduction with PCA

We can use PCA results to reduce the feature dimension of our dataset by projecting eah example onto a lower dimensional space.
In this part, we will project the example dataset into a 1-dimensional space.


#### 2.3.1 Make 'projectData' function
``` {r projectfunction}
projectData = function(X, U, K){
  # U indicates principal components and K means desired number of dimensions
  # to reduce.
  
  n = dim(X)[2] # number of features(dimensions)
  m = dim(X)[1] # number of example
  
  U_reduce = U[, 1 : K] # n by K
  Z = X %*% U_reduce # Z contains results of projection, m by K
  Z
}
```

#### 2.3.2 Implements projection
``` {r prjectimple}
K = 1
projection = projectData(X_norm, principal_components, K)
```

### 2.4 Reconstructing an approximation of the data

We can approximately recover the data by projecting them back onto the original high dimensional space.

#### 2.4.1 Make 'recoverData' function
``` {r recovdata}
recoverData = function(Z, U, K){
 X_rec = Z %*% t(U[, 1 : K])
 X_rec
}
```

#### 2.4.2 Reconstruct data
``` {r reconstruct}
recov = recoverData(projection, principal_components, K)
```

### 2.5 Visualizing the projections

We can see how the projection affects the data by visualization.
Note that we should use normalized dataset to see the effect of projection.

``` {r visualizeeffect}
# Make data.frame for projected dataset
projected_dataset = data.frame(V1 = recov[, 1], V2 = recov[, 2])

# Make data.frame for normalized sample dataset
n_example_dataset = data.frame(V1 = X_norm[, 1], V2 = X_norm[, 2])

p2 = ggplot(n_example_dataset, aes(V1, V2)) + geom_point(color = 'blue') +
  scale_x_continuous(breaks = seq(-4, 3, 1)) + 
  scale_y_continuous(breaks = seq(-4, 3, 1)) + theme_bw() +
  coord_fixed(ratio = 1) +
  ggtitle("The normalized and projected data after PCA") +
  theme(legend.title = element_blank(), panel.grid.major.x = element_blank() ,
        panel.grid.minor.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.x = element_blank())

p2 = p2 + geom_point(data = projected_dataset, aes(V1, V2), color = "red")
p2
```

The projection effectively only retains the information in the direction given by top principal component.


### 2.6 Face image dataset

In this part of the exercise, we will run PCA on face images to see how it can be used in practice for dimension reduction.

The dataset 'ex7faces.mat' contains a dataset X of face images, each 32 by 32 in grayscale. Each row of X corresponds to one face image

#### 2.6.1 Data loading and display the first 100 of these face images
``` {r facedata}
data = read.mat('C:/Users/user/Documents/Basic-ML-with_R/data/ex7faces.mat')
list2env(data, .GlobalEnv)
face = X # 5000 by 1024
rm(data)
```

we use 'displayData1' function to display face images.
special thanks to https://github.com/faridcher 
``` {r displaydata1}
# Function loading
source("C:/Users/user/Documents/Basic-ML-with_R/function/displayData1.R")
displayData1(face[1:100, ])
```

Now, implement PCA on facedata.

``` {r normalface}
# We first normalize the dataset
normalization = featureNormalize(face) 
face_norm = normalization$X_norm # 5000 by 1024
```

To implement PCA on large-scale dataset we make 'PCA1' function using 'svds' function in RSpectra package.
``` {r pca1function}
PCA1 = function(X, k) {
  # k means the number of eigenvalues(with the largest magnitude) to calculate
  m = dim(X)[1]
  n = dim(X)[2]
  
  # Make variance-covariance matrix for X
  v_c_matrix = (t(X) %*% X) / m
  
  # Eigenvalue decomposition v_c_matrix
  eigendecomp = svds(v_c_matrix, k, nu = k, nv = k)
  # RSpectra has the svds() function to compute Truncated SVD.
  # It returns eigenvectors u and eigenvalues d.
  eigendecomp
}
```

Implement PCA on face_norm matrix
```{r implepcaface}

# Loading RSpectra package to use 'svds' function in PCA1
library(RSpectra)

# Compare to 'PCA' function, 'PCA1' function has additional argument k which indicates
# the number of eigenvalues(with the largest maginitude) to calculate.
k = 100
eigenface = PCA1(face_norm, k)

# Save the results
principal_components = eigenface$u # saving first 100 eigenvectors, 1024 by 100
variances = eigenface$d # saving first 100 eigenvalues
```

Visualize the first 36 principal components that describe the largest variations
```{r visulapca}
principal_36 = t(principal_components[, 1:36]) # 36 by 1024
displayData1(principal_36)
```


#### 2.6.2 Dimensionality Reduction

We can use reduce the dimension of the face dataset.

Here, we project the face dataset onto only the first 100 principal components. Concretely, each face image is now described by a vector z(i) belongs to R^100.
```{r projectface}
projection_face = projectData(face_norm, principal_components, 100)
```

To understand what is lost in the dimension reduction, we can recover the data using the projected dataset.
```{r recovface}
recov_face = recoverData(projection_face, principal_components, 100)
```

Visualize an approximate recovery of the data and compare to the original data.
``` {r vizucompare}
op = par(mfrow = c(1, 2))

displayData1(face[1:100, ])
title('Original faces')

displayData1(recov_face[1:100, ])
title(main = 'Recovered faces')

par(op)
```
From the reconstruction, we can observe that the general structure and appearance of the face are kept while the fine details are lost.
