---
title: "Logistic regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1.Logistic regression without regularization term

### 1.1 Visualizing the data
```{r visualization}
data = read.table("C:/Users/user/Documents/Basic-ML-with_R/data/ex2data1.txt", sep = ',')
data_plot = data

# to make ggplot represent admission result
data_plot$V3 = ifelse(data$V3 == 1, 'Admitted', 'Not admitted')

# Plotting scatter plot
library(ggplot2)
p = ggplot(data_plot, aes(V1, V2)) + geom_point(aes(color = V3))+
    ylab("Exam 2 score") + xlab("Exam 1 score") +
    scale_x_continuous(breaks = seq(30, 100, 10)) +
    scale_y_continuous(breaks = seq(30, 100, 10)) +
    theme_bw() + ggtitle("Scatter plot of training data") +
    theme(legend.title = element_blank(), 
    panel.grid.major.x =element_blank(),
    panel.grid.minor.y = element_blank(),  
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.y = element_blank())   
p
```

### 1.2 Compute Cost J & Gradient

#### 1.2.1 Settings
```{r settings}
y = data[, 3]
m = length(y)
X = cbind(rep(1, m), data[, 1:2])
X = as.matrix(X) # convert X to matrix from data.frame

# initialize parameters
initial_theta = c(rep(0, dim(X)[2]))
lambda = 0 # we don't introce regularization term in this part.

```

#### 1.2.2 Computing the cost J(theta)
``` {r computing cost J}
costFunction = function(X, y, lambda) {
  function(theta) {
    J = 0
    m = length(y)
    X_theta = X %*% theta # m by 1
    h = 1 / (1 + exp(-X_theta)) # m by 1
    # [-1] syntex make vector without 1st element
    reg_term = (t(theta[-1]) %*% theta[-1]) * (lambda / 2)
    J = (t(y) %*% log(h) + t(1 - y) %*% (log(1 - h)) - reg_term) / (-m)
    J
  }
}

# Computing Cost J with initial_theta
sprintf('Cost J: %.3f', costFunction(X, y, lambda)(initial_theta))

```


#### 1.2.3 Computing Gradient for optimization
``` {r gradient}
computeGradient = function(X, y, lambda) {
  function(theta) {
    gradient = c(rep(0, dim(X)[2]))
    m = length(y)
    X_theta = X %*% theta
    h = 1 / (1 + exp(-X_theta))
    gradient = (t(X) %*% (h - y)) / m
    gradient_w_regular = gradient[-1] + ((lambda / m) * theta[-1])
    gradient = c(gradient[1], gradient_w_regular)
    gradient
  }
}

gradient = computeGradient(X, y, lambda)(initial_theta)

# Computing gradient with initial_theta
sprintf('gradient: %.3f, %.3f, %.3f: ', gradient[1], gradient[2], gradient[3])

```

### 1.3 Optimization

Learning parameters using advanced optimization algorithm
In this exercise, use a built-in function (optim) to find the
optimal parameters theta.

``` {r optimization}
# Run optim to obtain the optimal theta
# This function will return theta and the cost
optimResult = optim(par = initial_theta,
                 fn =  costFunction(X, y, lambda),
                 gr = computeGradient(X, y, lambda), 
                 method = "BFGS", control = list(maxit = 400))
# maxit is maximum iteration

theta = optimResult$par
cost = optimResult$value

# Printing result
sprintf('cost at theta found by optim: %.3f', cost)
sprintf('Optimized theta: %.3f %.3f %.3f:', theta[1], theta[2], theta[3])

```

### 1.4 Plotting decision boundary
``` {r decision boundary}
p = p + geom_abline(slope = - theta[2]/theta[3],
                    intercept = -theta[1]/theta[3], color = "blue")
p
```

### 1.5 Evaluation 

After learning the parameters, you'll like to use it to predict the outcomes on unseen data. 

``` {r eval}
# Predict probability for a student with score 45 on exam 1
# and score 85 on exam 2

predict_sample = t(c(1, 45, 85))
probability = 1 / (1 + exp(-predict_sample %*% theta))

sprintf('For a student with scores 45 and 85, we predict an admission probability of %.3f', probability)


# Compute accuracy on our training set
# Make predict function first
Predict = function(X, theta) {
  m = dim(X)[1]
  p = rep(0, m)
  
  p[X %*% theta >= 0 ] = 1
  p
}  

# make predict vector  
predict = Predict(X, theta)

accuracy = mean(predict == y) * 100
sprintf('Train Accuracy: %.3f:', accuracy)
```


# 2.Regularized logistic regression

### 1.1 Visualizing the data

